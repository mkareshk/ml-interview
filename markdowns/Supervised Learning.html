

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Supervised Learning &mdash; Machine Learning Interview Questions 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/theme_overrides.css" />

  
    <link rel="canonical" href="https://mkareshk.github.io/ml-interview/markdowns/Supervised%20Learning.html" />
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=d45e8c67"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Support Vector Machines" href="Support%20Vector%20Machines.html" />
    <link rel="prev" title="Semi-Supervised Learning" href="Semi-Supervised%20Learning.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Machine Learning Interview Questions
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Active%20Learning.html">Active Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Anomaly%20Detection.html">Anomaly Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="Attention%20Mechanisms.html">Attention Mechanisms</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bayesian%20Inference.html">Bayesian Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bayesian%20Neural%20Networks.html">Bayesian Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Clustering%20Algorithms.html">Clustering Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="Convolutional%20Neural%20Networks.html">Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Cross-Validation%20Techniques.html">Cross-Validation Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="Decision%20Trees.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="Dimensionality%20Reduction.html">Dimensionality Reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="Ensemble%20Methods.html">Ensemble Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="Ethics%20in%20Machine%20Learning.html">Ethics in Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Feature%20Engineering.html">Feature Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="Federated%20Learning.html">Federated Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Generative%20Adversarial%20Networks.html">Generative Adversarial Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Gradient%20Descent%20Variants.html">Gradient Descent Variants</a></li>
<li class="toctree-l1"><a class="reference internal" href="Graphical%20Models.html">Graphical Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Hyperparameter%20Tuning.html">Hyperparameter Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Interpretability%20and%20Explainability.html">Interpretability and Explainability</a></li>
<li class="toctree-l1"><a class="reference internal" href="K-Nearest%20Neighbors.html">K-Nearest Neighbors</a></li>
<li class="toctree-l1"><a class="reference internal" href="Linear%20Regression.html">Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="Logistic%20Regression.html">Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="Markov%20Decision%20Processes.html">Markov Decision Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="Meta-Learning.html">Meta-Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Model%20Evaluation%20Metrics.html">Model Evaluation Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="Naive%20Bayes.html">Naive Bayes</a></li>
<li class="toctree-l1"><a class="reference internal" href="Natural%20Language%20Processing.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="Neural%20Networks.html">Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Optimization%20Algorithms.html">Optimization Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="Quantum%20Machine%20Learning.html">Quantum Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Random%20Forests.html">Random Forests</a></li>
<li class="toctree-l1"><a class="reference internal" href="Recurrent%20Neural%20Networks.html">Recurrent Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Reinforcement%20Learning.html">Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Self-Supervised%20Learning.html">Self-Supervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Semi-Supervised%20Learning.html">Semi-Supervised Learning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Supervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Support%20Vector%20Machines.html">Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="Time%20Series%20Analysis.html">Time Series Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="Transfer%20Learning.html">Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Unsupervised%20Learning.html">Unsupervised Learning</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Machine Learning Interview Questions</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Supervised Learning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/markdowns/Supervised Learning.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="supervised-learning">
<h1>Supervised Learning<a class="headerlink" href="#supervised-learning" title="Link to this heading">ÔÉÅ</a></h1>
<hr class="docutils" />
<p><strong>Question:</strong> Analyze the impact of class imbalance on the predictive distribution of probabilistic classifiers.</p>
<p><strong>Answer:</strong>
Class imbalance significantly affects the predictive distribution of probabilistic classifiers. In imbalanced datasets, the classifier might become biased towards the majority class, leading to skewed probability estimates. For instance, consider a binary classifier trained on data where class 0 is much more frequent than class 1. The classifier might predict the probability <span class="math notranslate nohighlight">\(P(y=1|x)\)</span> to be lower than it should be for instances of the minority class, as it learns to minimize the overall error by favoring the majority class.</p>
<p>Mathematically, this can be understood by considering the likelihood function used in training. With imbalanced data, the likelihood is dominated by the majority class, leading to parameter estimates that do not adequately represent the minority class. This results in a predictive distribution that underestimates the minority class probabilities. Techniques like re-sampling, cost-sensitive learning, or using metrics like AUC can help mitigate these issues by rebalancing the influence of classes during training.</p>
<hr class="docutils" />
<p><strong>Question:</strong> How can semi-supervised learning techniques enhance label efficiency in supervised learning?</p>
<p><strong>Answer:</strong>
Semi-supervised learning (SSL) enhances label efficiency by leveraging both labeled and unlabeled data. In traditional supervised learning, models rely solely on labeled data, which can be costly and time-consuming to obtain. SSL, however, uses a small labeled dataset along with a larger set of unlabeled data to improve learning.</p>
<p>The key idea is to exploit the structure of the data distribution. Techniques like self-training, co-training, and generative models (e.g., variational autoencoders) allow the model to learn from the unlabeled data by assuming that similar instances share the same label. For instance, in self-training, a model is initially trained on the labeled data, then uses its predictions on the unlabeled data as pseudo-labels to further refine its learning.</p>
<p>Mathematically, SSL can be seen as optimizing a loss function that combines a supervised loss on labeled data <span class="math notranslate nohighlight">\(L_s\)</span> and an unsupervised loss <span class="math notranslate nohighlight">\(L_u\)</span> on unlabeled data, such as <span class="math notranslate nohighlight">\(L = L_s + \lambda L_u\)</span>, where <span class="math notranslate nohighlight">\(\lambda\)</span> balances the two components.</p>
<hr class="docutils" />
<p><strong>Question:</strong> What are the implications of the Vapnik-Chervonenkis dimension on the capacity of supervised learning models?</p>
<p><strong>Answer:</strong>
The Vapnik-Chervonenkis (VC) dimension is a measure of the capacity or complexity of a set of functions that a supervised learning model can implement. It is defined as the largest number of points that can be shattered by the hypothesis class, where ‚Äúshattering‚Äù means that the model can classify the points into all possible binary labelings.</p>
<p>A higher VC dimension indicates a more complex model capable of fitting more intricate patterns in the data. However, this also implies a greater risk of overfitting, as the model might capture noise rather than the underlying distribution. Conversely, a lower VC dimension suggests a simpler model that might underfit the data.</p>
<p>Mathematically, if a model has VC dimension <span class="math notranslate nohighlight">\(d_{VC}\)</span>, it can shatter any set of <span class="math notranslate nohighlight">\(d_{VC}\)</span> points but not necessarily any set of <span class="math notranslate nohighlight">\(d_{VC} + 1\)</span> points. Thus, VC dimension provides a theoretical bound on the model‚Äôs generalization ability.</p>
<hr class="docutils" />
<p><strong>Question:</strong> Discuss the impact of imbalanced class distribution on the performance of supervised learning classifiers.</p>
<p><strong>Answer:</strong>
Imbalanced class distribution significantly affects the performance of supervised learning classifiers. When one class is underrepresented, classifiers tend to be biased towards the majority class, leading to poor generalization on the minority class. This occurs because most learning algorithms aim to minimize overall error, which can be achieved by simply predicting the majority class most of the time.</p>
<p>Mathematically, consider a binary classification problem with classes <span class="math notranslate nohighlight">\(C_1\)</span> and <span class="math notranslate nohighlight">\(C_2\)</span>, where <span class="math notranslate nohighlight">\(P(C_1) \gg P(C_2)\)</span>. A classifier might achieve high accuracy by predicting <span class="math notranslate nohighlight">\(C_1\)</span> for all instances, but this would result in high false-negative rates for <span class="math notranslate nohighlight">\(C_2\)</span>.</p>
<p>Metrics like accuracy become misleading in such scenarios. Alternatives like precision, recall, F1-score, and area under the ROC curve (AUC-ROC) are more informative. Techniques such as resampling, cost-sensitive learning, and using algorithms inherently robust to imbalance (e.g., tree-based methods) can mitigate these effects.</p>
<hr class="docutils" />
<p><strong>Question:</strong> How does the choice of loss function impact a model‚Äôs robustness to outliers in supervised learning?</p>
<p><strong>Answer:</strong>
The choice of loss function significantly affects a model‚Äôs robustness to outliers in supervised learning. Loss functions determine how errors are penalized during training. For instance, the Mean Squared Error (MSE) loss function, <span class="math notranslate nohighlight">\(L(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\)</span>, squares the residuals, which means that larger errors (outliers) have a disproportionately large impact on the total loss. This makes models trained with MSE sensitive to outliers.</p>
<p>In contrast, the Mean Absolute Error (MAE), <span class="math notranslate nohighlight">\(L(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|\)</span>, penalizes errors linearly, reducing the influence of outliers. Robust loss functions like the Huber loss combine both approaches, being quadratic for small errors and linear for large ones, thus balancing sensitivity and robustness. Choosing a loss function aligned with the data‚Äôs characteristics and the model‚Äôs objectives is crucial for achieving robustness to outliers.</p>
<hr class="docutils" />
<p><strong>Question:</strong> Explain the role of surrogate loss functions in optimizing non-convex objectives in supervised learning.</p>
<p><strong>Answer:</strong>
In supervised learning, surrogate loss functions are used to optimize non-convex objectives by providing a tractable and often convex approximation of the original problem. Many machine learning tasks involve minimizing non-convex objectives, such as the 0-1 loss in classification, which is computationally intractable due to its discontinuous nature. Surrogate losses, like the hinge loss in SVMs or the cross-entropy loss in neural networks, offer smooth and differentiable alternatives that are easier to optimize using gradient-based methods. For instance, the hinge loss <span class="math notranslate nohighlight">\(\max(0, 1 - y f(x))\)</span> is a convex upper bound on the 0-1 loss, enabling efficient optimization while maintaining a relationship to the original objective. These surrogate functions facilitate convergence to a local minimum of the non-convex objective by leveraging the properties of convex optimization, thus playing a crucial role in practical learning algorithms.</p>
<hr class="docutils" />
<p><strong>Question:</strong> How does the choice of hypothesis space affect the learnability of a supervised learning model?</p>
<p><strong>Answer:</strong>
The hypothesis space in supervised learning is the set of functions the model can choose from to best represent the data. A larger hypothesis space increases the model‚Äôs capacity to fit complex patterns, but also risks overfitting, where the model captures noise rather than the underlying distribution. Conversely, a smaller hypothesis space may lead to underfitting, where the model fails to capture the data‚Äôs complexity.</p>
<p>Mathematically, the choice of hypothesis space affects the model‚Äôs VC (Vapnik‚ÄìChervonenkis) dimension, which quantifies the capacity of the model. A higher VC dimension implies a more complex model that can shatter more data points. The trade-off between bias and variance is central here: a complex hypothesis space reduces bias but increases variance, while a simpler space does the opposite.</p>
<p>For example, linear models have a limited hypothesis space, suitable for linear relationships, whereas neural networks have a vast hypothesis space, capable of modeling highly nonlinear functions.</p>
<hr class="docutils" />
<p><strong>Question:</strong> Analyze the effects of sample complexity on the generalization performance of supervised learning algorithms.</p>
<p><strong>Answer:</strong>
Sample complexity refers to the number of training samples required for a learning algorithm to achieve a desired level of generalization performance. In supervised learning, generalization performance is the ability of a model to perform well on unseen data. The sample complexity depends on factors like the hypothesis class complexity, the learning algorithm, and the desired accuracy and confidence levels.</p>
<p>Mathematically, for a hypothesis class <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> with VC-dimension <span class="math notranslate nohighlight">\(d_{VC}\)</span>, the sample complexity <span class="math notranslate nohighlight">\(m(\epsilon, \delta)\)</span> to achieve an error less than <span class="math notranslate nohighlight">\(\epsilon\)</span> with probability at least <span class="math notranslate nohighlight">\(1-\delta\)</span> is approximately <span class="math notranslate nohighlight">\(O\left(\frac{d_{VC} + \log(1/\delta)}{\epsilon^2}\right)\)</span>. Larger <span class="math notranslate nohighlight">\(d_{VC}\)</span> implies more samples are needed.</p>
<p>For example, a linear classifier in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> has <span class="math notranslate nohighlight">\(d_{VC} = d+1\)</span>, indicating that more features (higher <span class="math notranslate nohighlight">\(d\)</span>) increase sample complexity. Insufficient samples lead to overfitting, where the model captures noise rather than the underlying distribution, reducing generalization performance.</p>
<hr class="docutils" />
<p><strong>Question:</strong> How does the structure of hypothesis classes influence the generalization bounds in supervised learning?</p>
<p><strong>Answer:</strong>
The structure of hypothesis classes significantly influences generalization bounds in supervised learning. Generalization bounds quantify the difference between training error and expected error on unseen data. A key factor is the complexity of the hypothesis class, often measured by capacity metrics like VC dimension or Rademacher complexity.</p>
<p>A hypothesis class with high complexity can fit the training data well but may overfit, leading to poor generalization. Conversely, a simpler hypothesis class might underfit but generalize better. Mathematically, generalization bounds often take the form:</p>
<div class="math notranslate nohighlight">
\[ R(h) \leq \hat{R}(h) + \text{complexity term} + \epsilon \]</div>
<p>where <span class="math notranslate nohighlight">\(R(h)\)</span> is the expected risk, <span class="math notranslate nohighlight">\(\hat{R}(h)\)</span> is the empirical risk, and the complexity term depends on the hypothesis class. For example, in VC theory, the bound includes <span class="math notranslate nohighlight">\(\sqrt{\frac{d \log(n/d)}{n}}\)</span>, where <span class="math notranslate nohighlight">\(d\)</span> is the VC dimension and <span class="math notranslate nohighlight">\(n\)</span> is the number of samples. Thus, the hypothesis class structure directly impacts the trade-off between fitting the training data and maintaining generalization.</p>
<hr class="docutils" />
<p><strong>Question:</strong> How do ensemble learning techniques leverage the bias-variance decomposition in supervised learning?</p>
<p><strong>Answer:</strong>
Ensemble learning techniques, such as bagging and boosting, leverage the bias-variance decomposition to improve model performance in supervised learning. The bias-variance tradeoff describes how model complexity impacts prediction error: bias measures error due to overly simplistic assumptions, while variance measures error due to sensitivity to data fluctuations.</p>
<p>Bagging (Bootstrap Aggregating) reduces variance by averaging predictions from multiple models trained on different data subsets. For example, Random Forests use bagging to combine decision trees, reducing variance without significantly increasing bias.</p>
<p>Boosting, on the other hand, iteratively trains models, focusing on correcting errors from previous models. This process can reduce both bias and variance by creating a strong learner from weak learners. AdaBoost is a classic boosting algorithm that adjusts weights on training samples based on previous errors.</p>
<p>Mathematically, ensemble methods aim to minimize the expected error <span class="math notranslate nohighlight">\(E[(\hat{f}(x) - f(x))^2]\)</span>, where <span class="math notranslate nohighlight">\(\hat{f}\)</span> is the ensemble prediction and <span class="math notranslate nohighlight">\(f\)</span> is the true function, by balancing bias and variance contributions.</p>
<hr class="docutils" />
<p><strong>Question:</strong> What are the theoretical challenges in applying supervised learning to high-dimensional data spaces?</p>
<p><strong>Answer:</strong>
High-dimensional data spaces pose several theoretical challenges for supervised learning. One key issue is the ‚Äúcurse of dimensionality,‚Äù which refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces. As the number of dimensions increases, the volume of the space increases exponentially, making the data sparse. This sparsity makes it difficult to estimate statistical properties and requires exponentially more data to achieve the same level of accuracy as in lower dimensions.</p>
<p>Another challenge is overfitting, where models become too complex and fit the noise in the training data rather than the underlying distribution. In high dimensions, the number of parameters can be very large, increasing the risk of overfitting.</p>
<p>Moreover, distance metrics become less meaningful in high-dimensional spaces, as the distance between any two points tends to converge, affecting algorithms that rely on distance measures, such as k-nearest neighbors.</p>
<p>Mathematically, if <span class="math notranslate nohighlight">\(d\)</span> is the number of dimensions, the number of data points needed to maintain a constant density grows exponentially with <span class="math notranslate nohighlight">\(d\)</span>, leading to practical challenges in training models.</p>
<hr class="docutils" />
<p><strong>Question:</strong> How do ensemble methods enhance the performance of individual supervised learning models?</p>
<p><strong>Answer:</strong>
Ensemble methods enhance the performance of individual supervised learning models by combining multiple models to produce a more robust and accurate prediction. The key idea is to leverage the diversity among models to reduce variance, bias, or improve predictions.</p>
<p>For instance, in bagging (e.g., Random Forests), multiple models are trained on different subsets of the data, and their predictions are averaged, reducing variance. In boosting (e.g., AdaBoost), models are trained sequentially, with each new model focusing on the errors of the previous ones, reducing bias.</p>
<p>Mathematically, consider an ensemble prediction <span class="math notranslate nohighlight">\(\hat{f}(x) = \frac{1}{M} \sum_{m=1}^M \hat{f}_m(x)\)</span>, where <span class="math notranslate nohighlight">\(\hat{f}_m(x)\)</span> is the prediction from the <span class="math notranslate nohighlight">\(m\)</span>-th model. This averaging reduces the variance of the prediction, as <span class="math notranslate nohighlight">\(Var(\hat{f}(x)) = \frac{1}{M^2} \sum_{m=1}^M Var(\hat{f}_m(x))\)</span> if models are independent. Thus, ensemble methods often outperform individual models by balancing bias-variance trade-offs.</p>
<hr class="docutils" />
<p><strong>Question:</strong> Explain the role of regularization in mitigating overfitting within supervised learning models.</p>
<p><strong>Answer:</strong>
Regularization is a technique used to prevent overfitting in supervised learning models by adding a penalty term to the loss function. Overfitting occurs when a model captures noise in the training data, resulting in poor generalization to unseen data. Regularization discourages overly complex models by penalizing large weights.</p>
<p>Common regularization techniques include L1 (Lasso) and L2 (Ridge) regularization. L1 regularization adds the sum of the absolute values of the weights to the loss function, promoting sparsity:</p>
<div class="math notranslate nohighlight">
\[ L_{L1} = \sum_{i=1}^{n} |w_i|. \]</div>
<p>L2 regularization adds the sum of the squared values of the weights, encouraging smaller weights:</p>
<div class="math notranslate nohighlight">
\[ L_{L2} = \sum_{i=1}^{n} w_i^2. \]</div>
<p>These penalties help control the model‚Äôs complexity, leading to better generalization. For instance, in linear regression, the regularized loss function becomes:</p>
<div class="math notranslate nohighlight">
\[ L = \text{Loss}(y, \hat{y}) + \lambda L_{\text{reg}}, \]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is the regularization strength.</p>
<hr class="docutils" />
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Semi-Supervised%20Learning.html" class="btn btn-neutral float-left" title="Semi-Supervised Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="Support%20Vector%20Machines.html" class="btn btn-neutral float-right" title="Support Vector Machines" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Moein Kareshk.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>