

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Unsupervised Learning &mdash; Machine Learning Interview Questions 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/theme_overrides.css" />

  
    <link rel="canonical" href="https://mkareshk.github.io/ml-interview/markdowns/Unsupervised%20Learning.html" />
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=d45e8c67"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Transfer Learning" href="Transfer%20Learning.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Machine Learning Interview Questions
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Active%20Learning.html">Active Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Anomaly%20Detection.html">Anomaly Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="Attention%20Mechanisms.html">Attention Mechanisms</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bayesian%20Inference.html">Bayesian Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bayesian%20Neural%20Networks.html">Bayesian Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Clustering%20Algorithms.html">Clustering Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="Convolutional%20Neural%20Networks.html">Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Cross-Validation%20Techniques.html">Cross-Validation Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="Decision%20Trees.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="Dimensionality%20Reduction.html">Dimensionality Reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="Ensemble%20Methods.html">Ensemble Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="Ethics%20in%20Machine%20Learning.html">Ethics in Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Feature%20Engineering.html">Feature Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="Federated%20Learning.html">Federated Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Generative%20Adversarial%20Networks.html">Generative Adversarial Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Gradient%20Descent%20Variants.html">Gradient Descent Variants</a></li>
<li class="toctree-l1"><a class="reference internal" href="Graphical%20Models.html">Graphical Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Hyperparameter%20Tuning.html">Hyperparameter Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Interpretability%20and%20Explainability.html">Interpretability and Explainability</a></li>
<li class="toctree-l1"><a class="reference internal" href="K-Nearest%20Neighbors.html">K-Nearest Neighbors</a></li>
<li class="toctree-l1"><a class="reference internal" href="Linear%20Regression.html">Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="Logistic%20Regression.html">Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="Markov%20Decision%20Processes.html">Markov Decision Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="Meta-Learning.html">Meta-Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Model%20Evaluation%20Metrics.html">Model Evaluation Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="Naive%20Bayes.html">Naive Bayes</a></li>
<li class="toctree-l1"><a class="reference internal" href="Natural%20Language%20Processing.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="Neural%20Networks.html">Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Optimization%20Algorithms.html">Optimization Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="Quantum%20Machine%20Learning.html">Quantum Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Random%20Forests.html">Random Forests</a></li>
<li class="toctree-l1"><a class="reference internal" href="Recurrent%20Neural%20Networks.html">Recurrent Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Reinforcement%20Learning.html">Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Self-Supervised%20Learning.html">Self-Supervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Semi-Supervised%20Learning.html">Semi-Supervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Supervised%20Learning.html">Supervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Support%20Vector%20Machines.html">Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="Time%20Series%20Analysis.html">Time Series Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="Transfer%20Learning.html">Transfer Learning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Unsupervised Learning</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Machine Learning Interview Questions</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Unsupervised Learning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/markdowns/Unsupervised Learning.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="unsupervised-learning">
<h1>Unsupervised Learning<a class="headerlink" href="#unsupervised-learning" title="Link to this heading">ÔÉÅ</a></h1>
<hr class="docutils" />
<p><strong>Question:</strong> How does density-based clustering handle the identification of noise versus clusters in complex datasets?</p>
<p><strong>Answer:</strong>
Density-based clustering, such as DBSCAN (Density-Based Spatial Clustering of Applications with Noise), identifies clusters by looking for regions of high point density and distinguishing them from regions of low density, which are considered noise.</p>
<p>In DBSCAN, two parameters are crucial: <span class="math notranslate nohighlight">\(\epsilon\)</span> (epsilon) and <span class="math notranslate nohighlight">\(minPts\)</span> (minimum number of points). A point is considered a core point if there are at least <span class="math notranslate nohighlight">\(minPts\)</span> points within its <span class="math notranslate nohighlight">\(\epsilon\)</span>-neighborhood. Clusters are formed by connecting core points that are within <span class="math notranslate nohighlight">\(\epsilon\)</span> distance of each other. Points that are not reachable from any core point are labeled as noise.</p>
<p>This method effectively identifies clusters of arbitrary shape and handles noise by not assigning it to any cluster. For example, in a dataset with two dense regions separated by sparse noise, DBSCAN will identify the dense regions as clusters and the sparse region as noise.</p>
<hr class="docutils" />
<p><strong>Question:</strong> How does the choice of similarity measure affect the performance of spectral clustering algorithms?</p>
<p><strong>Answer:</strong>
The choice of similarity measure in spectral clustering significantly impacts performance, as it defines the affinity matrix <span class="math notranslate nohighlight">\(A\)</span> used to capture the relationships between data points. Common measures include Gaussian kernels and cosine similarity. The Gaussian kernel, defined as <span class="math notranslate nohighlight">\(A_{ij} = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)\)</span>, emphasizes local neighborhood structures, with the bandwidth parameter <span class="math notranslate nohighlight">\(\sigma\)</span> controlling sensitivity to distances. Cosine similarity, <span class="math notranslate nohighlight">\(A_{ij} = \frac{x_i \cdot x_j}{\|x_i\|\|x_j\|}\)</span>, is effective for high-dimensional data where only directional relationships matter. An inappropriate choice can lead to poor clustering, as it may not capture the intrinsic data structure, leading to an ill-formed Laplacian matrix <span class="math notranslate nohighlight">\(L = D - A\)</span>, where <span class="math notranslate nohighlight">\(D\)</span> is the degree matrix. This affects the eigenvalues and eigenvectors used for clustering, potentially resulting in suboptimal partitions.</p>
<hr class="docutils" />
<p><strong>Question:</strong> Discuss the role of mutual information in measuring the effectiveness of representation learning in unsupervised settings.</p>
<p><strong>Answer:</strong>
In unsupervised representation learning, mutual information (MI) quantifies the dependency between input data <span class="math notranslate nohighlight">\(X\)</span> and its learned representation <span class="math notranslate nohighlight">\(Z\)</span>. MI measures how much information about <span class="math notranslate nohighlight">\(X\)</span> is captured by <span class="math notranslate nohighlight">\(Z\)</span>, providing a metric for the effectiveness of the representation. High MI indicates that <span class="math notranslate nohighlight">\(Z\)</span> retains significant information about <span class="math notranslate nohighlight">\(X\)</span>, which is desirable for tasks like clustering or anomaly detection.</p>
<p>Mathematically, mutual information is defined as:</p>
<div class="math notranslate nohighlight">
\[ I(X; Z) = \int \int p(x, z) \log \frac{p(x, z)}{p(x)p(z)} \, dx \, dz \]</div>
<p>where <span class="math notranslate nohighlight">\(p(x, z)\)</span> is the joint probability distribution of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Z\)</span>, and <span class="math notranslate nohighlight">\(p(x)\)</span> and <span class="math notranslate nohighlight">\(p(z)\)</span> are their marginal distributions.</p>
<p>In practice, maximizing MI encourages representations that preserve the underlying structure of data. However, estimating MI directly is challenging, leading to approximations like variational bounds. These approximations guide the learning process to enhance the quality of representations without explicit labels.</p>
<hr class="docutils" />
<p><strong>Question:</strong> Discuss the limitations of using K-means for clustering data with varying cluster sizes and densities.</p>
<p><strong>Answer:</strong>
K-means clustering assumes that clusters are spherical and of similar size, which limits its effectiveness when dealing with clusters of varying sizes and densities. The algorithm minimizes the within-cluster variance, which can lead to poor performance if clusters vary significantly in size or density. For example, K-means uses the Euclidean distance to assign points to clusters, which can result in smaller, denser clusters being absorbed by larger, sparser ones. Additionally, K-means is sensitive to outliers, which can skew the mean and affect cluster assignments. Mathematically, K-means aims to minimize the objective function <span class="math notranslate nohighlight">\(J = \sum_{i=1}^{k} \sum_{x \in C_i} \| x - \mu_i \|^2\)</span>, where <span class="math notranslate nohighlight">\(C_i\)</span> is the set of points in cluster <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(\mu_i\)</span> is the centroid. This approach does not account for varying cluster shapes or densities, making it unsuitable for data with such characteristics.</p>
<hr class="docutils" />
<p><strong>Question:</strong> How do generative models in unsupervised learning capture underlying data distributions without explicit labels?</p>
<p><strong>Answer:</strong>
Generative models in unsupervised learning aim to capture the underlying data distribution <span class="math notranslate nohighlight">\(p(x)\)</span> without relying on explicit labels. They do this by learning a model <span class="math notranslate nohighlight">\(p_\theta(x)\)</span> parameterized by <span class="math notranslate nohighlight">\(\theta\)</span> that approximates the true data distribution. Techniques such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) are popular approaches.</p>
<p>VAEs learn a latent variable model by maximizing the evidence lower bound (ELBO), which involves a trade-off between reconstruction accuracy and the complexity of the latent space. The objective is to maximize <span class="math notranslate nohighlight">\(\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) \| p(z))\)</span>, where <span class="math notranslate nohighlight">\(q_\phi(z|x)\)</span> is the approximate posterior.</p>
<p>GANs, on the other hand, use a min-max game between a generator <span class="math notranslate nohighlight">\(G\)</span> and a discriminator <span class="math notranslate nohighlight">\(D\)</span>. The generator tries to produce data that the discriminator cannot distinguish from real data, optimizing <span class="math notranslate nohighlight">\(\min_G \max_D \mathbb{E}_{x \sim p_{data}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]\)</span>. These models learn to generate data that resembles the training set, effectively capturing its distribution.</p>
<hr class="docutils" />
<p><strong>Question:</strong> How does spectral clustering utilize eigenvectors to identify non-convex clusters in data?</p>
<p><strong>Answer:</strong>
Spectral clustering is a technique that leverages the eigenvectors of a data similarity matrix to identify clusters, particularly effective for non-convex shapes. The process starts by constructing a similarity graph from the data, where nodes represent data points and edges reflect similarities. The Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span> of this graph is computed, which encodes the structure of the data.</p>
<p>The key step is to compute the eigenvectors of <span class="math notranslate nohighlight">\(L\)</span>. The eigenvectors corresponding to the smallest eigenvalues capture the most significant structure of the data. By selecting the top <span class="math notranslate nohighlight">\(k\)</span> eigenvectors, we embed the data into a lower-dimensional space where clusters are more distinct.</p>
<p>Finally, a standard clustering algorithm like k-means is applied in this new space. The spectral embedding effectively separates non-convex clusters by considering the global structure of the data, which is not possible with traditional distance-based methods.</p>
<hr class="docutils" />
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Transfer%20Learning.html" class="btn btn-neutral float-left" title="Transfer Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Moein Kareshk.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>